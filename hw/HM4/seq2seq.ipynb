{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 4: Build a seq2seq model for machine translation.\n",
    "\n",
    "    ### Name: Andrew Chen\n",
    "\n",
    "### Task: Translate English to French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code.\n",
    "2. Complete the code in Section 1.1 and Section 4.2.\n",
    "\n",
    "    * Translation English to **German** is not acceptable!!! Try another language.\n",
    "    \n",
    "3. **Make improvements.** Directly modify the code in Section 3. Do at least one of the followings. By doing more, you will get up to 2 bonus scores to the total.\n",
    "\n",
    "    * Bi-LSTM instead of LSTM\n",
    "    \n",
    "    * Multi-task learning (e.g., both English to French and English to Spanish)\n",
    "    \n",
    "    * Attention\n",
    "    \n",
    "4. Evaluate the translation using the BLEU score. \n",
    "\n",
    "    * Optional. Up to 2 bonus scores to the total.\n",
    "    \n",
    "5. Convert the notebook to .HTML file. \n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "\n",
    "6. Put the .HTML file in your own Github repo. \n",
    "\n",
    "7. Submit the link to the HTML file to Canvas\n",
    "\n",
    "    * E.g., https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM4/seq2seq.html\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "To implement ```Bi-LSTM```, you will need the following code to build the encoder; the decoder won't be much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Bidirectional, Concatenate\n",
    "\n",
    "# encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "#                                   dropout=0.5, name='encoder_lstm'))\n",
    "# _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "# state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "To implement multi-task training, you can refer to ```Section 7.1.3 Multi-output models``` of the textbook, ```Deep Learning with Python```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the following blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename = \"Data/fra.txt\"\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = 30000\n",
    "\n",
    "rand_indices = numpy.random.permutation(n_train)\n",
    "train_indices = rand_indices[0:20000]\n",
    "valid_indices = rand_indices[20000:25000]\n",
    "test_indices = rand_indices[25000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc = load_doc(filename)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs = to_pairs(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_data(pairs)[train_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[he squinted] => [il a louche]\n",
      "[does it offend you] => [cela vous offensetil]\n",
      "[did he go there] => [y estil alle]\n",
      "[be prepared] => [soyez pretes]\n",
      "[she baked me a cake] => [elle me prepara un gateau]\n",
      "[i feel it now] => [je le sens maintenant]\n",
      "[she is darkskinned] => [elle a la peau noire]\n",
      "[lets not watch tv] => [ne regardons pas la television]\n",
      "[youre talkative] => [tu es bavard]\n",
      "[i hate fanatics] => [je deteste les fanatiques]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs[i, 0] + '] => [' + clean_pairs[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (20000,)\n",
      "Length of target_texts: (20000,)\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_pairs[:, 0]\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_pairs[:, 1]]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input  sentences: 19\n",
      "max length of target sentences: 56\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts)\n",
    "max_decoder_seq_length = max(len(line) for line in target_texts)\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 19)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (20000, 56)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tjaurai besoin de votre aide\\n'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 19,  4,  8, 12,  4,  5,  2, 21,  1,  3,  9,  5,  7,  2, 18,  1,\n",
       "        2, 17,  9,  6, 12,  1,  2,  4,  5, 18,  1, 11,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 19, 28)\n",
      "(20000, 56, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = numpy.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, None, 28)          0         \n",
      "_________________________________________________________________\n",
      "encoder_lstm (LSTM)          [(None, 256), (None, 256) 291840    \n",
      "=================================================================\n",
      "Total params: 291,840\n",
      "Trainable params: 291,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "# SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "# plot_model(\n",
    "#     model=encoder_model, show_shapes=False,\n",
    "#     to_file='encoder.pdf'\n",
    "# )\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 301,598\n",
      "Trainable params: 301,598\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "#SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "#plot_model(\n",
    "#    model=decoder_model, show_shapes=False,\n",
    "#    to_file='decoder.pdf'\n",
    "#)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_lstm_16/while/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"decoder_input_h_8:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(state_h)\n",
    "print(decoder_input_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 256), (None, 291840      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 593,438\n",
      "Trainable params: 593,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "#SVG(model_to_dot(model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "#plot_model(\n",
    "#    model=model, show_shapes=False,\n",
    "#    to_file='model_training.pdf'\n",
    "#)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 19, 28)\n",
      "shape of decoder_input_data(20000, 56, 30)\n",
      "shape of decoder_target_data(20000, 56, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks=[EarlyStopping(patience=5, monitor='val_loss'),\n",
    "            ModelCheckpoint(filepath='save' + \"/\" + 'seq2seq_second.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                            monitor='val_loss', verbose=0, mode='auto', period=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/300\n",
      "16000/16000 [==============================] - 23s 1ms/step - loss: 1.1646 - val_loss: 0.9024\n",
      "Epoch 2/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.8797 - val_loss: 0.7606\n",
      "Epoch 3/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.8008 - val_loss: 0.7078\n",
      "Epoch 4/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.7489 - val_loss: 0.6784\n",
      "Epoch 5/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.7109 - val_loss: 0.6402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_8/encoder_lstm/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_8/encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.6803 - val_loss: 0.6096\n",
      "Epoch 7/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.6539 - val_loss: 0.5901\n",
      "Epoch 8/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.6432 - val_loss: 0.5715\n",
      "Epoch 9/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.6194 - val_loss: 0.5536\n",
      "Epoch 10/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5995 - val_loss: 0.5375\n",
      "Epoch 11/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5835 - val_loss: 0.5274\n",
      "Epoch 12/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5691 - val_loss: 0.5098\n",
      "Epoch 13/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5574 - val_loss: 0.4991\n",
      "Epoch 14/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5466 - val_loss: 0.4870\n",
      "Epoch 15/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5345 - val_loss: 0.4780\n",
      "Epoch 16/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5256 - val_loss: 0.4725\n",
      "Epoch 17/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5161 - val_loss: 0.4610\n",
      "Epoch 18/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.5072 - val_loss: 0.4560\n",
      "Epoch 19/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4996 - val_loss: 0.4482\n",
      "Epoch 20/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4919 - val_loss: 0.4395\n",
      "Epoch 21/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4837 - val_loss: 0.4352\n",
      "Epoch 22/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4783 - val_loss: 0.4292\n",
      "Epoch 23/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4718 - val_loss: 0.4281\n",
      "Epoch 24/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4658 - val_loss: 0.4212\n",
      "Epoch 25/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4603 - val_loss: 0.4164\n",
      "Epoch 26/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4548 - val_loss: 0.4110\n",
      "Epoch 27/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4509 - val_loss: 0.4084\n",
      "Epoch 28/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4457 - val_loss: 0.4049\n",
      "Epoch 29/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4408 - val_loss: 0.4028\n",
      "Epoch 30/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4371 - val_loss: 0.3978\n",
      "Epoch 31/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4327 - val_loss: 0.3959\n",
      "Epoch 32/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4292 - val_loss: 0.3922\n",
      "Epoch 33/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4245 - val_loss: 0.3883\n",
      "Epoch 34/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4220 - val_loss: 0.3875\n",
      "Epoch 35/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4179 - val_loss: 0.3855\n",
      "Epoch 36/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4144 - val_loss: 0.3822\n",
      "Epoch 37/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4113 - val_loss: 0.3797\n",
      "Epoch 38/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4077 - val_loss: 0.3782\n",
      "Epoch 39/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4053 - val_loss: 0.3763\n",
      "Epoch 40/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4023 - val_loss: 0.3726\n",
      "Epoch 41/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4003 - val_loss: 0.3714\n",
      "Epoch 42/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3963 - val_loss: 0.3705\n",
      "Epoch 43/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3947 - val_loss: 0.3688\n",
      "Epoch 44/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3926 - val_loss: 0.3660\n",
      "Epoch 45/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3893 - val_loss: 0.3640\n",
      "Epoch 46/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3868 - val_loss: 0.3637\n",
      "Epoch 47/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3848 - val_loss: 0.3635\n",
      "Epoch 48/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3821 - val_loss: 0.3610\n",
      "Epoch 49/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3804 - val_loss: 0.3598\n",
      "Epoch 50/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3785 - val_loss: 0.3598\n",
      "Epoch 51/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3763 - val_loss: 0.3555\n",
      "Epoch 52/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3737 - val_loss: 0.3554\n",
      "Epoch 53/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3715 - val_loss: 0.3541\n",
      "Epoch 54/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3704 - val_loss: 0.3527\n",
      "Epoch 55/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3676 - val_loss: 0.3529\n",
      "Epoch 56/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3669 - val_loss: 0.3541\n",
      "Epoch 57/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3644 - val_loss: 0.3509\n",
      "Epoch 58/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3625 - val_loss: 0.3518\n",
      "Epoch 59/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3598 - val_loss: 0.3480\n",
      "Epoch 60/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3591 - val_loss: 0.3473\n",
      "Epoch 61/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.3572 - val_loss: 0.3467\n",
      "Epoch 62/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3559 - val_loss: 0.3472\n",
      "Epoch 63/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3537 - val_loss: 0.3436\n",
      "Epoch 64/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3534 - val_loss: 0.3435\n",
      "Epoch 65/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3514 - val_loss: 0.3431\n",
      "Epoch 66/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3494 - val_loss: 0.3447\n",
      "Epoch 67/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3486 - val_loss: 0.3429\n",
      "Epoch 68/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.3457 - val_loss: 0.3423\n",
      "Epoch 69/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3441 - val_loss: 0.3408\n",
      "Epoch 70/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3443 - val_loss: 0.3386\n",
      "Epoch 71/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3426 - val_loss: 0.3386\n",
      "Epoch 72/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3413 - val_loss: 0.3382\n",
      "Epoch 73/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3410 - val_loss: 0.3391\n",
      "Epoch 74/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3396 - val_loss: 0.3401\n",
      "Epoch 75/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3363 - val_loss: 0.3380\n",
      "Epoch 76/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.3358 - val_loss: 0.3387\n",
      "Epoch 77/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3353 - val_loss: 0.3367\n",
      "Epoch 78/300\n",
      "16000/16000 [==============================] - 24s 2ms/step - loss: 0.3329 - val_loss: 0.3373\n",
      "Epoch 79/300\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.3324 - val_loss: 0.3359\n",
      "Epoch 80/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.3313 - val_loss: 0.3346\n",
      "Epoch 81/300\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.3303 - val_loss: 0.3342\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.3292 - val_loss: 0.3334\n",
      "Epoch 83/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3282 - val_loss: 0.3350\n",
      "Epoch 84/300\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.3271 - val_loss: 0.3342\n",
      "Epoch 85/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3243 - val_loss: 0.3317\n",
      "Epoch 86/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3248 - val_loss: 0.3311\n",
      "Epoch 87/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3238 - val_loss: 0.3333\n",
      "Epoch 88/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3232 - val_loss: 0.3332\n",
      "Epoch 89/300\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.3218 - val_loss: 0.3318\n",
      "Epoch 90/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3199 - val_loss: 0.3322\n",
      "Epoch 91/300\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.3195 - val_loss: 0.3329\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data],  # training data\n",
    "          decoder_target_data,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=300, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "### 4.1. Translate English to XXX\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        come on admit it\n",
      "French (true):  allez admettezle\n",
      "French (pred):  allez aider moi tout le monde\n",
      "-\n",
      "English:        i need to do this\n",
      "French (true):  il me faut le faire\n",
      "French (pred):  jai besoin de vous aller tout les deux\n",
      "-\n",
      "English:        you may go\n",
      "French (true):  tu peux ten aller\n",
      "French (pred):  vous pouvez vous aider\n",
      "-\n",
      "English:        i owe you a lunch\n",
      "French (true):  je vous dois un dejeuner\n",
      "French (pred):  je vous dois un peu de main\n",
      "-\n",
      "English:        i want a low table\n",
      "French (true):  je veux une table basse\n",
      "French (pred):  je veux un peu de main\n",
      "-\n",
      "English:        it tasted sweet\n",
      "French (true):  ca goutait sucre\n",
      "French (pred):  ca a pris un peu de main\n",
      "-\n",
      "English:        we found something\n",
      "French (true):  on a trouve quelque chose\n",
      "French (pred):  nous lavons vu un aller\n",
      "-\n",
      "English:        it was just hype\n",
      "French (true):  cetait juste du battage publicitaire\n",
      "French (pred):  cetait tres bien\n",
      "-\n",
      "English:        she drives me crazy\n",
      "French (true):  elle me rend chevre\n",
      "French (pred):  elle me lent de la porte\n",
      "-\n",
      "English:        its a world record\n",
      "French (true):  cest un record du monde\n",
      "French (pred):  cest une bonne idee\n",
      "-\n",
      "English:        didnt you go out\n",
      "French (true):  netesvous pas sortis\n",
      "French (pred):  netesvous pas serieux\n",
      "-\n",
      "English:        advance two steps\n",
      "French (true):  avance de deux pas\n",
      "French (pred):  ajune ma voiture de maintenant\n",
      "-\n",
      "English:        tom keeps his word\n",
      "French (true):  tom tient parole\n",
      "French (pred):  tom a pris de magnaise\n",
      "-\n",
      "English:        ive never met her\n",
      "French (true):  je ne lai jamais rencontree\n",
      "French (pred):  je ne lai jamais vue\n",
      "-\n",
      "English:        youll need this\n",
      "French (true):  tu auras besoin de ca\n",
      "French (pred):  tu as lair en colere\n",
      "-\n",
      "English:        whos tom\n",
      "French (true):  cest qui tom\n",
      "French (pred):  qui est ca\n",
      "-\n",
      "English:        i checked my bags\n",
      "French (true):  jai enregistre mes bagages\n",
      "French (pred):  jai fait ce que je dis\n",
      "-\n",
      "English:        wheres the knife\n",
      "French (true):  ou est le couteau\n",
      "French (pred):  ou est la maison\n",
      "-\n",
      "English:        we try our best\n",
      "French (true):  nous faisons de notre mieux\n",
      "French (pred):  nous sommes tous deux bonnes\n",
      "-\n",
      "English:        are you blushing\n",
      "French (true):  rougissezvous\n",
      "French (pred):  estu serieux\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2100, 2120):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('English:       ', input_texts[seq_index])\n",
    "    print('French (true): ', target_texts[seq_index][1:-1])\n",
    "    print('French (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: why is that\n",
      "translated sentence is: estu serieux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'why is that'\n",
    "\n",
    "input_sequences, _ = text2sequences(len(input_sentence), [input_sentence])\n",
    "\n",
    "translated_sentence = decode_sequence(input_seq)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "\n",
    "\n",
    "**Hint:** Randomly partition the dataset to training, validation, and test. Evaluate the BLEU score using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i want to quit' 'je veux demissionner']\n",
      "-\n",
      "source sentence is: i want to quit\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: je veux demissionner\n",
      "score: 0\n",
      "-\n",
      "source sentence is: its not a loan\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: ce nest pas un pret\n",
      "score: 0\n",
      "-\n",
      "source sentence is: come back here\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: revenez ici\n",
      "score: 0\n",
      "-\n",
      "source sentence is: were sunk\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: on est foutu\n",
      "score: 0\n",
      "-\n",
      "source sentence is: were really good\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: nous sommes vraiment bons\n",
      "score: 0\n",
      "-\n",
      "source sentence is: dont wait for me\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: ne mattendez pas\n",
      "score: 0\n",
      "-\n",
      "source sentence is: turn it off\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: eteinsle\n",
      "score: 0\n",
      "-\n",
      "source sentence is: i love the sun\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: jadore le soleil\n",
      "score: 0\n",
      "-\n",
      "source sentence is: this box is light\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: cette caisse est legere\n",
      "score: 0\n",
      "-\n",
      "source sentence is: tom is voting\n",
      "translated sentence is: estu serieux\n",
      "\n",
      "true traslation: tom est en train de voter\n",
      "score: 0\n",
      "\n",
      "AVERAGE SCORE: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# reference from first BLEU link\n",
    "\n",
    "# reference = [['the', 'cat', 'is', 'on', 'the', 'mat']]\n",
    "# candidate = ['the', 'cat', 'is', 'on', 'cat']\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "# print(score)\n",
    "\n",
    "\n",
    "# use test rand indices to test on BLEU\n",
    "test_pairs = clean_data(pairs)[test_indices, :]\n",
    "\n",
    "print(test_pairs[0])\n",
    "\n",
    "n_bleu_tests = 10\n",
    "\n",
    "score_total = 0\n",
    "\n",
    "for i in range(n_bleu_tests):\n",
    "    # get translation\n",
    "    input_sentence = test_pairs[i][0]\n",
    "    input_sequences, _ = text2sequences(len(input_sentence), [input_sentence])\n",
    "    translated_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    # get translation's bleu score\n",
    "    reference = [test_pairs[i][0].split()]\n",
    "    candidate = translated_sentence.split()\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    score_total += score\n",
    "    \n",
    "    print('-')\n",
    "    print('source sentence is: ' + input_sentence)\n",
    "    print('translated sentence is: ' + translated_sentence)\n",
    "    print('true traslation:', test_pairs[i][1])\n",
    "    print('score:', score)\n",
    "\n",
    "print(\"\\nAVERAGE SCORE:\", score_total / n_bleu_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
